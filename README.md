# CNN-Multiclass-Classification

Start date:
August 29th, 4:03 PM

Goal:
To create a "filter" for the larger language model for text generation to ensure its output is not too inappropriate for a situation given the context. This can be done via text classification, in which bert excels in as opposed to language generating models like ChatGPT. I'm considering making this project in a way that will result in a feedback loop between the models, hopefully helping the language generation model generates better text

Current Progress:
-Learning How to stack transformer decoders
-trying to find relevant data I can use to train the model

To Do:
Finish coding out the decoders, encoders (incase it is needed), and learn how the bert architecture works (since all I know right now is that it is a stack of decoders in the transformer architecture)
