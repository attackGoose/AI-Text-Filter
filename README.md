# CNN-Multiclass-Classification

Start date:
August 29th, 4:03 PM

Goal:
To create a "filter" for the larger language model for text generation to ensure its output is not too bad. This can be done via text classification, in which bert excels in as opposed to language generating models like ChatGPT

Current Progress:
Learning How to stack transformer decoders


To Do:
Finish coding out the decoders, encoders (incase it is needed) and learn how the bert architecture works (since all I know right now is that it is a stack of decoders in the transformer architecture)
